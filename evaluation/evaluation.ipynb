{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Evaluation via GPT-4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing DS modules\n",
    "import pandas as pd\n",
    "\n",
    "## Importing OpenAI modules\n",
    "import openai\n",
    "\n",
    "## Importing other modules\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Path for evaluation output\n",
    "TEST_SAVE_PATH = \"/Users/kaanaydin/Library/CloudStorage/OneDrive-SharedLibraries-UniversitätSt.Gallen/STUD-NLP Group Project - General/02 Group project/evaluation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set your OpenAI API key\n",
    "openai.api_key = ''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OpenAI setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to evaluate answer given question & retrieve score and explanation from GPT-4\n",
    "\n",
    "def evaluate(question, answer):\n",
    "    \n",
    "    ## Develop prompt based on answer & question. Prompt inspired by Vicuna team's original prompt\n",
    "    prompt = f\"You are a helpful and precise assistant for checking the quality of the answer.\\n Question:'{question}' \\n Response: '{answer}' \\n We would like to request your feedback on the performance of the AI assistants in response to the user question displayed above.\\n Please rate the helpfulness, relevance, level of details of the responses. Each answer should receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\n Please first output the score on a scale from 1 to 10 followed by '-|-' as separator and a comprehensive explanation of your evaluation, avoiding any potential bias\"\n",
    "\n",
    "    ## Structure message in GPT-4 format\n",
    "    message=[{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    ## Generate the response from OpenAI GPT-4\n",
    "    response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4\",\n",
    "                temperature=0.2,\n",
    "                messages = message)\n",
    "\n",
    "    ## Retrieve answer from resopnse\n",
    "    answer = response['choices'][0]['message']['content']\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selected model name to be evaluated \n",
    "\n",
    "selected_model_name = \"gpt-3.5-davinci\"\n",
    "#selected_model_name = \"gelectra-finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read data from your excel file\n",
    "test = pd.read_csv(os.path.join(TEST_SAVE_PATH, f'{selected_model_name}_evaluation.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lists to capture the scores and the evaluations\n",
    "scores = []\n",
    "explanations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Iterating over all rows from the testset\n",
    "\n",
    "for row in test.itertuples():\n",
    "    \n",
    "    ## Retrieving question and answer from the testset\n",
    "    question = str(row.Frage)\n",
    "    answer = str(row.ModelAntwort)\n",
    "\n",
    "    ## Querying model for evaluation\n",
    "    evaluation = evaluate(question, answer)\n",
    "    \n",
    "    ## Retrieving and splitting up the score and explanation\n",
    "    score = evaluation.split(\"-|-\")[0]\n",
    "    explanation = evaluation.split(\"-|-\")[1]\n",
    "\n",
    "    ## Appending the score and explanation to the list\n",
    "    scores.append(score)\n",
    "    explanations.append(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IN CASE THE CODE ABOVE BREAKS DUE TO OPENAI OVERLOADED WITH REQUESTS\n",
    "\n",
    "## Define the breaking point where OpenAI was overloaded with requests\n",
    "breakingpoint = min(len(scores), len(explanations))\n",
    "\n",
    "## Make sure the list of scores and explanations have the same length\n",
    "scores = scores[0:breakingpoint]\n",
    "explanations = explanations[0:breakingpoint]\n",
    "\n",
    "## Create new df dropping all rows that have already been processed\n",
    "test2 = test.iloc[breakingpoint:]\n",
    "\n",
    "\n",
    "## Iterating over all rows from the remaining teset\n",
    "for row in test2.itertuples():\n",
    "    \n",
    "    ## Retrieving question and answer from the testset\n",
    "    question = str(row.Frage)\n",
    "    answer = str(row.ModelAntwort)\n",
    "\n",
    "    ## Querying model for evaluation\n",
    "    evaluation = evaluate(question, answer)\n",
    "    \n",
    "    ## Retrieving and splitting up the score and explanation\n",
    "    score = evaluation.split(\"-|-\")[0]\n",
    "    explanation = evaluation.split(\"-|-\")[1]\n",
    "\n",
    "    ## Appending the score and explanation to the list\n",
    "    scores.append(score)\n",
    "    explanations.append(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating copy of the testeset\n",
    "evaluation = copy.deepcopy(test)\n",
    "\n",
    "## Adding scores and explanations to the testset\n",
    "evaluation[\"AutoEvalScore\"] = scores\n",
    "evaluation[\"AutoEvalExplan\"] = explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving testset\n",
    "evaluation.to_csv(os.path.join(TEST_SAVE_PATH, f'{selected_model_name}_evaluation.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eurosat-10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
